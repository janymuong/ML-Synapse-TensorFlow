# Wraps:
I've gotten a **grounding** in how to do **Natural Language Processing** with TensorFlow and Keras. I went from *first principles* -- basic `Tokenization` and `Padding` of text to produce data structures that could be used in a Neural Network. 

I then learned about **embeddings**, and how words could be mapped to *vectors*, and *words of similar semantics given vectors pointing in a similar direction*, giving you a mathematical model for their meaning, which could then be fed into a **deep neural network** for classification.

From there I started learning about `sequence models`, and how they help deepen my understanding of sentiment in text by not just looking at words in isolation, but also how their meanings change when they qualify one another. 

I am wrapping up by taking everything I learned and using it to build a poetry generator!