 # Embeddings

 Moving beyond text data processing ranging from pre-processing, reading from raw files and tokenizing text.

 ### Working w/ Text Embeddings.

 **`Embeddings`** means `tokens` are mapped as **vectors** in a high dimension space. With Embeddings and labelled examples, these vectors can then be tuned so that words with similar meaning will have a similar direction in the **vector space**. This will begin the process of training a neural network to understand sentiment in text - and we'll begin by looking at IMDB movie reviews, *training a neural network* on texts that are labelled '`positive`' or '`negative`' and determining which words in a sentence drive those meanings.  

 <p align="center">
  <img src="img/positive-negative.png" alt="Positive-Negative" height="200" style="display: inline-block; margin-right: 10px;">
  <img src="img/embed.png" alt="Embed" height="200" style="display: inline-block;">
</p>
