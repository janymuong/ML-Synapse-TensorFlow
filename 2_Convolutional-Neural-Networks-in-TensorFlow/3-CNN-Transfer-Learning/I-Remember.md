## What I Have Seen
This week I've learned a lot of great new concepts!

- I saw `Transfer Learning`, and how I can take an existing model, freeze many of its layers to prevent them being retrained, and effectively 'remember' the convolutions it was trained on to fit images. 

- I added a DNN underneath a base model so that I could retrain on my images using the convolutions from the other model. 

- I learned about regularization using dropout to make a network more efficient in preventing over-specialization and thus overfitting.